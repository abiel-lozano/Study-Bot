# Study-Bot - studyBot.py

>This documentation was created for version 1.0 of Study-Bot, in December 2023

## Description

This script contains all of Study-Bot's functionalities, and when it is run as a standalone script, it will serve as a command-line interface for the user to interact with the bot. This is used mostly for testing new features and debugging and is not really intended for production usage. 

Note that this documentation was created with the intent of helping a student-level developer understand the code to the point of being able to contribute to the project.

## Usage

It is recommended to use [Python 3.9.9](https://www.python.org/downloads/release/python-399/), since the `whisper` library may present compatibility issues with newer versions of Python. If you want to avoid having to remove your current Python installation, you may want to set up a virtual enviroment to use this specific version of Python. 

To install the required dependencies, run the following command:

```bash
pip install -r requirements.txt
```

The `whisper` library and the audio playback functionality from `elevenlabs` requires the **ffmpeg** command line tool to be installed. You can download the executable directly from [here](https://github.com/BtbN/FFmpeg-Builds/releases). Alternatively, you could install it with [Chocolatey](https://chocolatey.org/), a package manager for Windows, by running the following command:

```bash
choco install ffmpeg
```

Make sure that the folder `ffmpeg/bin`, which contains the **ffmpeg** executable, is added to the **PATH environment variables**.

To use this script, simply run the following command within the root directory of the project. Consider that you need to have the API keys set up in the `credentials.py` file before running the script, please refer to the `credentials-template.py` file.

```bash
python src/studyBot.py
```

## Code Walkthrough

### Imports

```python
import openai
import whisper
from pydub import AudioSegment
from pydub.playback import play as pydubPlay
import io
from typing import Iterator
import pyaudio
import wave
from pathlib import Path
# History, is_installed, and subprocess are used only by main script
from elevenlabs import set_api_key, generate, History, is_installed, subprocess
import cv2
import numpy as np
import time
import threading
import keyboard
import credentials
import sourceMaterial

```
- `openai`: Used to access the API to access the `gpt-3.5-turbo-16k` model
- `whisper`: Used to access the API for the **speech-to-text** conversion
- `pydub`: Used to stream the audio as it is generated by the **text-to-speech** conversion service
- `io`: Used for in-memory input/output operations, treating data as bytes. Provides a stream interface for working with binary data in memory to allow audio streaming
- `pyaudio`: Abstracts the complexities of audio input/output operations, allowing us to easily record audio from the microphone
- `wave`: Used to save audio recordings in the `.wav` format
- `pathlib`: Allows us to easily work with file paths, used to delete the audio recordings after they are no longer needed
- `elevenlabs`: Used to access the API for the **text-to-speech** conversion service
- `cv2`: Computer vision library used to detect the physical educational material that the user is holding using color
- `numpy`: Used to work with arrays, which are used to store the pixel data of the images
- `time`: Allows the use of timed functions, used to control how long the bot will look for objects
- `threading`: Used to perform multiple tasks at the same time using **multithreading**, allowing the bot to look for objects while the user is asking their question
- `keyboard`: Used to detect keyboard inputs to control the bot
- `credentials`: Contains the API keys as variables for the services used by the bot. Note that this file is not included in the repository for security reasons, and must be created manually by the contributor/developer based on the template provided in `src/credentials-template.py`
- `sourceMaterial`: Contains variables with the source material for the topics supported by Study-Bot.
---
### Initialize Variables and API Configuration

Global variables are used to avoid using input parameters and return values for most of our functions, as this complicates things when the functions are called asynchronously using multithreading.

Here is where the API keys should be set after creating the `credentials.py` file. You can obtain an API key from **OpenAI** [here](https://platform.openai.com/account/api-keys), and an API key from **ElevenLabs** [here](https://docs.elevenlabs.io/api-reference/quick-start/authentication). These sites also provide additional information on how to use the APIs, and it is highly recommended to read the documentation before making any changes on how they are used in this project.

```python
global objects
global question
global answer
global topic
global stop
objects = ''
question = ''
answer = ''
stop = False

GPT_MODEL = 'gpt-3.5-turbo-16k'

# Credentials
openai.api_key = credentials.openAIKey
set_api_key(credentials.elevenLabsKey)
```

Set up the custom instructions that **GPT** should follow when answering questions. This is essential for controlling its behavior and making sure that it answers the questions in a way that is useful for the user. You may need to modify these often considering that some updates to the model have changed the way it interprets the instructions in the past.

```python
instructions = """
Try to use the information below to help the user...
"""
```
---
### Audio Recording: `recordQuestion()`

This function records audio until the user stops it and creates a file named `question.wav` and sends it through the Whisper API to create a transcription of the audio. The `stopRecording()` function is used to break out of the while loop, and is called by a user input.

While you could select the language Whisper should expect as a parameter for the `model.transcribe()` function, ignoring this parameter as below will make Whisper detect the language automatically, which is a desireable feature to avoid having to select the language every time the bot is used. Note that this feature can lead to language misidentification in noisy environments or when using a low-quality microphone.

```python
CHUNK = 1024 # Chunk size
FORMAT = pyaudio.paInt16 # Audio codec format
CHANNELS = 2
RATE = 44100 # Sample rate
OUTPUT_FILE = 'question.wav'

def recordQuestion():
	global question
	global stop

	stop = False
	audio = pyaudio.PyAudio() # Initialize PyAudio
	# Open audio stream for recording
	stream = audio.open(format = FORMAT, channels = CHANNELS, rate = RATE, input = True, frames_per_buffer = CHUNK)
	frames = []

	# Record audio stream in chunks
	while not stop:
		data = stream.read(CHUNK)
		frames.append(data)

	# Stop and close audio stream
	stream.stop_stream()
	stream.close()
	audio.terminate()

	# Save recording as WAV
	wf = wave.open(OUTPUT_FILE, 'wb')
	wf.setnchannels(CHANNELS)
	wf.setsampwidth(audio.get_sample_size(FORMAT))
	wf.setframerate(RATE)
	wf.writeframes(b''.join(frames))
	wf.close()

	# STT Conversion
	model = whisper.load_model('base')
	result = model.transcribe(OUTPUT_FILE, fp16 = False)
	question = result['text']
	
	# Delete audio file
	Path(OUTPUT_FILE).unlink()

def stopRecording():
	global stop
	stop = True
```

---
### Object Recognition: `lookForObjects()`

*Different sets* of educational materials require *different methods* of object recognition, so the function is selected accordingly. The function `lookForObjects()` is called when the user asks a question, and it will populate the object list with the objects that the user is holding. This  list is a simple string containing natural language, as it needs to be read by **GPT**. The function `colorID()` is used to detect the objects for the human body set based on color, and the function `markerID()` is used to detect the objects for the Krebs cycle set based on the ArUco markers attached to the objects.

```python
def lookForObjects(topic: int):
	global objects
	objects = ''

	if topic == 1:
		# Call the function for color identification
		objects = colorID()
	elif topic == 2:
		# Call the function for marker identification
		objects = markerID()
```

**Color Identification:** `colorID()`

This function uses [OpenCV](https://opencv.org/) to detect objects that match any of the predefined color ranges. It will look for objects for one second, and if it finds any, it will add to the `objects` variable what it has found. If it doesn't find anything, the variable will contain the default message, set at the start of the function.

For each of the objects you wish to identify, set a lower and upper bound with the lightest and darkest values for the color of the object in the **HSV color space**. You may use a color picker tool to obtain these values. Consider that the effectiveness of these values will depend on the lighting conditions of the environment, color accuracy of the camera, white balance, and other factors.

>Note: For **OpenCV**, the H (hue) value is represented as a value between 0 and 180, instead of the standard 0 to 360, and the S (saturation) and V (vibrancy) values are represented as values between 0 and 255 instead of 0 to 100.

<!-- Use white balance of 3154 for our camera -->
```python
def colorID():
	obj = 'User is not holding any objects'

	# Capture video
	cam = cv2.VideoCapture(0, cv2.CAP_DSHOW) # Use 0 for default camera

	# Start timer
	startTime = time.time()
	elapsedTime = 0

	# Color ranges
	stomachLower = np.array([90, 	80, 	    100       ], np.uint8)
	stomachUpper = np.array([120, 	255, 	    255       ], np.uint8)
	colonLower = np.array(	[10, 	255 * 0.55, 255 * 0.35], np.uint8)
	colonUpper = np.array(	[19.5, 	255, 	    255       ], np.uint8)
	liverLower = np.array(	[38, 	225 * 0.22, 255 * 0.38], np.uint8)
	liverUpper = np.array(	[41, 	255, 	    255       ], np.uint8)
	brainLower = np.array(	[161, 	255 * 0.50, 255 * 0.40], np.uint8)
	brainUpper = np.array(	[161, 	255, 	    255       ], np.uint8)
	kidneyLower = np.array(	[26, 	255 * 0.60, 255 * 0.69], np.uint8)
	kidneyUpper = np.array(	[26, 	255, 	    255       ], np.uint8)
	heartLower = np.array(	[179, 	255 * 0.50, 255 * 0.35], np.uint8)
	heartUpper = np.array(	[179, 	255 * 0.97, 255 * 0.69], np.uint8)

	while elapsedTime < 1:

		_, imageFrame = cam.read()

		# Convert frame from BGR color space to HSV
		hsvFrame = cv2.cvtColor(imageFrame, cv2.COLOR_BGR2HSV)

		# Create masks for each organ
		colonMask = cv2.inRange(hsvFrame, colonLower, colonUpper)
		liverMask = cv2.inRange(hsvFrame, liverLower, liverUpper)
		stomachMask = cv2.inRange(hsvFrame, stomachLower, stomachUpper)
		brainMask = cv2.inRange(hsvFrame, brainLower, brainUpper)
		kidneyMask = cv2.inRange(hsvFrame, kidneyLower, kidneyUpper)
		heartMask = cv2.inRange(hsvFrame, heartLower, heartUpper)
```

When initializing the camera, using the `CAP_DSHOW` flag reduces the time it takes to open the camera by a considerable amount. This is because the flag tells OpenCV to use a different API to access the camera, which is faster than the default API.

For each object, create binary masks where color-matched pixels are white and non-matching are black. To minimize false negatives, apply a dilation morphological transformation. This enlarges object boundaries in the binary image by sliding a structuring element (kernel) over the image and replacing each pixel with the maximum pixel value within the kernel's neighborhood.

A 5x5 square-shaped kernel was used for most organs, and a 12x12 for the kidney, since it was harder to detect it's specific color range. This fills gaps in the binary masks, making the objects more solid and continuous, and improving detection. 

Use the bitwise AND operation with binary masks to extract regions of interest from the original image. If both corresponding pixels are non-zero (white in the binary mask), the output image pixel retains its original color; otherwise, it turns black.

```python
		# Create a 5x5 square-shaped filter called kernel
		# Filter is filled with ones and will be used for morphological 
		# transformations such as dilation for better detection
		kernel = np.ones((5, 5), 'uint8')

		# For colon
		# Dilate mask: Remove holes in the mask by adding pixels to the 
		# boundaires of the objects in the mask
		colonMask = cv2.dilate(colonMask, kernel)
		# Apply mask to frame by using bitwise AND operation
		resColon = cv2.bitwise_and(imageFrame, imageFrame, mask = colonMask)

		# For liver
		liverMask = cv2.dilate(liverMask, kernel)
		resliver = cv2.bitwise_and(imageFrame, imageFrame, mask=liverMask)

		# For stomach
		stomachMask = cv2.dilate(stomachMask, kernel)
		resStomach = cv2.bitwise_and(imageFrame, imageFrame, mask=stomachMask)

		# For brain
		brainMask = cv2.dilate(brainMask, kernel)
		resBrain = cv2.bitwise_and(imageFrame, imageFrame, mask=brainMask)

		# For heart
		heartMask = cv2.dilate(heartMask, kernel)
		resHeart = cv2.bitwise_and(imageFrame, imageFrame, mask=heartMask)

		# For kidney use a more aggressive kernel for dilation
		kidneyMask = cv2.dilate(kidneyMask, np.ones((12, 12), 'uint8'))
		resKidney = cv2.bitwise_and(imageFrame, imageFrame, mask=kidneyMask)
```

Create a contour around the zone that matches the color range of the object. The binary masks generated earlier are used as the input to the `cv2.findContours()` function to find contours in the image that correspond to the color range of the object.

The function returns a list of contours, then, iterate over each contour found in the previous step using a loop, determine its `area`, and compare ir to a predefined **size threshold**.

If the `area` is greater than the threshold, it indicates that the detected region is significant enough to be considered a positive detection for the object, so its name is added to the list, while avoiding repeats. This operation is performed for each object of the set.

```python
		# Create a contour around the zone that matches the color range
		contours, hierarchy = cv2.findContours(colonMask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)
		# For each countour, check if the area is greater than the threshold
		for pic, contour in enumerate(contours):
			area = cv2.contourArea(contour)
			if area > 700:
				# Append the name of the model to the list of objects
				if 'colon' not in obj:
					if obj == 'User is not holding any objects':
						obj = 'colon'
					else:
						obj = obj + ', colon'

		contours, hierarchy = cv2.findContours(liverMask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)
		for pic, contour in enumerate(contours):
			area = cv2.contourArea(contour)
			if area > 500:
				if 'liver' not in obj:
					if obj == 'User is not holding any objects':
						obj = 'liver'
					else:
						obj = obj + ', liver'

		contours, hierarchy = cv2.findContours(stomachMask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)
		for pic, contour in enumerate(contours):
			area = cv2.contourArea(contour)
			if area > 1400:
				if 'stomach' not in obj:
					if obj == 'User is not holding any objects':
						obj = 'stomach'
					else:
						obj = obj + ', stomach'

		contours, hierarchy = cv2.findContours(brainMask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)
		for pic, contour in enumerate(contours):
			area = cv2.contourArea(contour)
			if area > 2500:
				if 'brain' not in obj:
					if obj == 'User is not holding any objects':
						obj = 'brain'
					else:
						obj = obj + ', brain'
		
		contours, hierarchy = cv2.findContours(heartMask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)
		for pic, contour in enumerate(contours):
			area = cv2.contourArea(contour)
			if area > 650:
				if 'heart' not in obj:
					if obj == 'User is not holding any objects':
						obj = 'heart'
					else:
						obj = obj + ', heart'

		contours, hierarchy = cv2.findContours(kidneyMask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)
		for pic, contour in enumerate(contours):
			area = cv2.contourArea(contour)
			if area > 50:
				if 'kidney' not in obj:
					if obj == 'User is not holding any objects':
						obj = 'kidney'
					else:
						obj = obj + ', kidney'
```

To calibrate this method of object detection to a specific set of objects, refer to the script located in `Tools/colorDetection.py` to find the color ranges that work best for the set of models and test using different `area` thresholds.

A properly calibrated set of color ranges and area thresholds should yield a level of detection accuracy resembling the image below, in which there is no false positives between similar colors and the amount of areas detected (or rectangles shown) for each object is close or equal to one.

<img src="Images/Color Detection.png" alt="Color Detection" width="400" align="center"/>

>Image generated using the `colorDetection.py` script, which added rectangles around the areas of the image which met the criteria. These educational models are 3D prints which represent some of the organs of the human body.

**ArUco Marker identification:** `markerID()`	

Consider that the ArUco module is not included in the default OpenCV installation, so you need to install a specific module named `opencv-contrib-python`. This is included in the `requirements.txt` file, but you can install it manually as follows: 

```bash
pip install opencv-contrib-python
```

This function uses **OpenCV's** ArUco module to detect the ArUco markers attached to the objects of a set. Similar to `colorID()`, it will look for the markers for *5* seconds, and if it finds any, it will add to the `objects` variable what it has found, and will leave the default message if it doesn't find anything.

**OpenCV** has predefined dictionaries of ArUco markers, and the `DICT_4X4_50` dictionary is used here, which contains 50 different markers of 4x4 bits. To obtain the images with the markers, refer to the script in `Tools/ArUcoCreate.py`.

Select a dictionary and asociate the number of an ArUco marker with the name of the object it is attached to:

```python
def markerID():
	obj = 'User is not holding any objects'

	# Choose the predefined dictionary to use
	arucoDict = cv2.aruco.getPredefinedDictionary(cv2.aruco.DICT_4X4_50)

	# Define the names of the objects
	compoundDict = { 0: 'Citrate', 1: 'Isocitrate', 2: 'Alpha-Ketoglutarate', 3: 'Succinyl CoA', 4: 'Succinate', 5: 'Fumarate', 6: 'Malate', 7: 'Oxaloacetate' }

	cap = cv2.VideoCapture(0, cv2.CAP_DSHOW) # Use 0 for default camera

	# Start timer
	startTime = time.time()
	elapsedTime = 0
```

For each captured frame, convert it to grayscale and use the `cv2.aruco.detectMarkers()` function to get a matrix of the detected markers and their corresponding IDs. If any markers are detected, iterate over the matrix and add the name of the object to the list, while avoiding repeats.

```python
	while elapsedTime < 5:
		ret, frame = cap.read()

		if not ret:
			print('Failed to capture frame.')
			break

		# Convert the frame to grayscale for marker detection
		gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)

		# Detect markers
		corners, ids, _ = cv2.aruco.detectMarkers(gray, arucoDict)

		if ids is not None:
			# For each marker found in current frame
			for i in range(len(ids)):
				try:
					# Try to get the name of the compound from the dictionary
					compoundName = compoundDict[ids[i][0]]
					# Append compound to list while avoiding repeats
					if obj == 'User is not holding any objects':
						obj = compoundName
					elif compoundName not in obj:
						obj += ', ' + compoundName
				except KeyError:
					continue

		# Display the frame
		cv2.imshow('Study-Bot View', frame)

		elapsedTime = time.time() - startTime

		# Check for 'Esc' key press
		key = cv2.waitKey(10) & 0xFF
		if key == 27:
			break
```

In this case the frame is displayed since it is more important to have the camera feed visible to the user, or the person assisting them, to make sure that the objects are correctly aligned with the camera when scanning for markers rather than detecting colors.

---
### Answer Generation: `sendMessage()`

This function simply takes a message list as input and sends it to the **OpenAI API**. Use the chat completions endpoint to allow follow up questions to previous messages. Consider that `ChatCompletions` is no longer available on the latest versions of the `openai` library. **Study-Bot** was developed using version `0.27.8`, and this specific version is indicated in the `requirements.txt` file; it can also be installed manually as:

```bash
pip install openai==0.27.8
```

The `temperature` parameter is used to control the 'randomness' of the response. A lower temperature value will result in more predictable responses, as the model is said to take a more objective and factual approach to the response.

After the answer is extracted from the API's JSON response, it is appended to `messageList` globally. Note that the answer is saved as a dictionary with the role of the message (`assistant` or `user`), as this is important information for the model to understand who sent which message when generating a response.

```python
def sendMessage(messageList: any):
	# Send prompt to GPT
	response = openai.ChatCompletion.create(
		messages = messageList,
		model = GPT_MODEL, 
		temperature = 0.2
	)
	# print(response)
	_answer = response['choices'][0]['message']['content']

	# Add the response to the message list
	messageList.append({'role': 'assistant', 'content': _answer})
```
---
### Text to Speech: `convertTTS()` and `streamAnswer()`

To convert **GPT's** text answer into speech, call the **Elevenlabs API** to generate the audio. Here, the `eleven_multilingual_v1` model is used, which automatically detects the language of the input text and generates the audio in the same language as long as it is supported. 

The `stream` parameter is set as True to play the audio as it is being generated, instead of waiting for the entire text to be converted to begin audio playback. The audio is streamed to the `streamAnswer()` function, which plays the audio using the `pydub` library.

>Note: There is a function in the **Elevenlabs** library for audio streaming, however, a different one is needed since the library's function requires the use of **mpv**, a command line media player. While it might be the better option for using this API through Python, it is not ideal for our specific use case, since we want non-technical users to be able to use the program without installing *too many* additional dependencies.

```python
def streamAnswer(audioStream: Iterator[bytes]) -> bytes:
	audioOutput = b''
	
	# For each chunk of audio in stream, add it to the output
	for chunk in audioStream:
		if chunk is not None:
			audioOutput += chunk

	# Play audio output using PyDub
	audioSegment = AudioSegment.from_file(io.BytesIO(audioOutput), format="mp3")
	pydubPlay(audioSegment)

def convertTTS(text: str):
	audioOutput = generate(text = text, model = 'eleven_multilingual_v1', stream = True)
	streamAnswer(audioOutput)
```
---
### Start Conversation and Use of Multithreading

This section of the code is only executed when `studyBot.py` is run as a standalone script. 

Use this as a topic selector to choose the source material for the conversation:

```python
	# Listen for keyboard input to stop recording
	keyboard.add_hotkey('s', stopRecording)

	print('Select a topic NUMBER from the list:\n')
	print('[1] - Human Body')
	print('[2] - Krebs Cycle\n')
	topic = int(input('Topic: '))
	source = ''

	# Load the source material based on the selected topic
	if topic == 1:
		print('Topic: Human Body\n')
		source = sourceMaterial.humanBody
	elif topic == 2:
		print('Topic: Krebs Cycle\n')
		source = sourceMaterial.krebsCycle
```

Transcribing audio into text and detecting objects are very time-consuming tasks. To avoid having to wait for one to finish before the other can start, use **multithreading** to run both processes simultaneously. Create two threads for these processes and wait for them to finish before continuing with the rest of the program.

```python
	# Start question processing threads
	objID = threading.Thread(target = lookForObjects, args = (topic,))
	audioRec = threading.Thread(target = recordQuestion)

	objID.start()
	print('Looking for objects...\n')
	audioRec.start()
	print('Listening for question...\n')

	objID.join()
	print('Object detection complete.\n')
	print('Objects detected: ' + objects + '\n')
	audioRec.join()
	print('Question recorded.\n')
	print('Question: ' + question + '\n')
```

After the information is gathered, build the initial query. Since this is the very first message sent to **GPT**, it includes the behavioral guidelines in `instructions` and the source material of the set of models. 

```python
	# Build prompt
	query = f"""{instructions}

	Objects held by user: {objects}.
	Question: {question}

	Information: 
	\"\"\"
	{source}
	\"\"\"
	"""
```
Notice the role parameters on the messages in `messageHistory`. The very first message has the `system` role, and serves as some initial context or instructions about the task that **GPT** will be performing in this conversation.

```python
	# Send prompt to GPT
	messageHistory = [
		{'role': 'system', 'content': 'You answer questions in the same language as the question.'},
		{'role': 'user', 'content': query},
	]

	print('Sending prompt to GPT...\n')
	sendMessage(messageHistory)
	# Get the answer from the last message in the message history
	answer = next((msg for msg in reversed(messageHistory) if msg['role'] == 'assistant'), None)['content']
	print(answer + '\n')
	
	if answer != '':
		print('Answer: ' + answer + '\n\n')

	# Convert answer to audio
	print('Converting answer to audio...\n')
	convertTTS(answer)
```
---
### Conversation Handling

After this initial message, the conversation is handled in a loop. The process is pretty straightforward and similar to the first message, with the main difference being the use of the keyboard to trigger the question processing, and in the use of the `sendMessage()` function, we append only the new messages without any need for additional context.

```python
	# Conversation loop, handles any follow-up questions
	while True:
		print('Press space to ask another question, or press q to quit.\n')

		while True:
			if keyboard.is_pressed(' '):
				print('Preparing for next question, please hold...\n')
				break
			if keyboard.is_pressed('q'):
				print('Exiting program...\n')
				exit()

		# Reset variables
		objects = 'User is not holding any objects'
		question = ''

		# Restart threads

		objID = threading.Thread(target = lookForObjects, args = (topic,))
		audioRec = threading.Thread(target = recordQuestion)

		objID.start()
		print('Looking for objects...\n')
		audioRec.start()
		print('Listening for question...\n')

		objID.join()
		print('Object detection complete.\n')
		print('Objects detected: ' + objects + '\n')
		audioRec.join()
		print('Question recorded.\n')
		print('Question: ' + question + '\n')

		# Build new prompt and add to chat history
		query = f"""Objects held by user: {objects}.
Question: {question}
"""
		messageHistory.append({'role': 'user', 'content': query})
		answer = ''

		# Send prompt to GPT
		# print('Prompt: ' + query + '\n') # For debugging only
		print('Sending prompt to GPT...\n')

		sendMessage(messageHistory)
		answer = next((msg for msg in reversed(messageHistory) if msg['role'] == 'assistant'), None)['content']

		if answer != '':
			print('Answer: ' + answer + '\n\n')

		print('Converting answer to audio...\n')
		convertTTS(answer)
```

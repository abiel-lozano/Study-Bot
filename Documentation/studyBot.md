# Study-Bot - studyBot.py

>This documentation was created for version 1.4.5 of Study-Bot, in June 2024.

## Description

This script contains all of Study-Bot's core functionality, and when it is run as a standalone script, it will serve as a command-line interface for the bot. This is used mostly for testing new features or debugging, so is not really intended for regular use.

Note that this documentation was created with the intent explaining reasoning behind the decisions made in the code, and to provide a general understanding of how the bot works for contributors of any level of expertise.

## Usage

**Study-Bot** was developed with [Python 3.9.9](https://www.python.org/downloads/release/python-399/), due to previous compatibility issues with newer versions of Python caused by a dependency which is no longer used. So, while not strictly necessary, it is still recommended to use this version to run the project. A virtual environment can be used to avoid having to remove your current installation of Python.

To install the required dependencies, run the following command:

```bash
pip install -r requirements.txt
```

Before running the project, consider that you must to have the API keys set up in the `credentials.py` file before running the script, using  `credentials-template.py` as a reference.

You can get an API key from **OpenAI** [here](https://platform.openai.com/account/api-keys), and an API key from **ElevenLabs** [here](https://docs.elevenlabs.io/api-reference/quick-start/authentication). These sites also provide additional information on how to use the APIs, and it is highly recommended to read their documentation before making any changes on how they are used in this project.

To use this (CLI-only) script, simply run the following command within the root directory of the project:

```bash
python src/studyBot.py
```

## Code Walkthrough

### Imports

```python
import pyaudio
import wave
import time
from pathlib import Path
from openai import OpenAI
import cv2
import numpy as np
from elevenlabs import play
from elevenlabs.client import ElevenLabs
import threading
import keyboard
import credentials # Contains API keys, create your own credentials.py file
import sourceMaterial
```
- `pyaudio`: Abstracts the complexities of audio input/output operations, allowing us to easily record audio from the microphone and play the generated audio from the text-to-speech service
- `wave`: Used to save audio recordings in the `.wav` format
- `time`: Allows the use of timed functions, used to control the length of the audio recording, and how long the bot should look for objects
- `pathlib`: Allows us to easily work with file paths, used to delete the audio recordings after they have been transcribed
- `openai`: Used to access the API for the **Whisper** speech-to-text model and **GPT** for answering questions.
- `cv2`: Computer vision library used to detect objects based on predefined color ranges and ArUco markers, and accessing the camera for the custom computer vision model.
- `numpy`: Used to work with arrays and matrices in the color detection method
- `elevenlabs`: Used to access the API for the text-to-speech conversion service
- `threading`: Used to perform multiple tasks at the same time, allowing the bot to look for objects while the user is asking their question, and keep the UI responsive while all other processes are running
- `keyboard`: Used to detect keyboard inputs to control the bot in the CLI version
- `credentials`: Contains the API keys as variables for the services used by the bot. This file is not included in the repository, and must be created manually by the contributor/user based on the template provided in `src/credentials-template.py`
- `sourceMaterial`: Contains variables with the source material for the topics supported by Study-Bot. For testing purposes, the source material was generated by ChatGPT.
---
### API Configurations

Initialize the clients for the **OpenAI** and **ElevenLabs** APIs by retrieving the keys from `credentials`.

```python
# Credentials
openAIClient = OpenAI(api_key = credentials.openAIKey)
elevenLabsClient = ElevenLabs(api_key = credentials.elevenLabsKey)
```

Set up the custom instructions that **GPT** should follow when answering questions. This is essential for controlling its behavior and making sure that it answers the questions in a way that is useful for the user. You may need to modify these often considering that some of the updates have previously changed how closely the model follows the instructions.

```python
instructions = """
You are a tutor helping a student...
"""
```
---
### Audio Recording: `recordQuestion()`

This function records audio until the user stops it and creates a file named `question.wav` and sends it through the OpenAI API to create a transcription of the audio. The recording is stopped by user input.

You could select the language Whisper should expect from the input audio as a parameter for the `openAIClient.audio.transcriptions.create()` function; ignoring this parameter will make Whisper try to detect the language automatically, which is useful to avoid having to select the language every time the bot is used. Note that this feature can lead to language misidentification in noisy environments and/or when using a low-quality microphone.

```python
# Recorder configuration
CHUNK = 1024 # Chunk size
FORMAT = pyaudio.paInt16 # Audio codec format
CHANNELS = 2
RATE = 44100 # Sample rate
OUTPUT_FILE = 'question.wav'

def recordQuestion():
  global question
  global stop
  global startTime

  startTime = time.time()
  stop = False

  audio = pyaudio.PyAudio()
  stream = audio.open(format = FORMAT, channels = CHANNELS, rate = RATE, input = True, frames_per_buffer = CHUNK)
  frames = []

  # Record audio stream in chunks
  while not stop:
    data = stream.read(CHUNK)
    frames.append(data)

  # Stop and close audio stream
  stream.stop_stream()
  stream.close()
  audio.terminate()

  # Save recording as WAV
  wf = wave.open(OUTPUT_FILE, 'wb')
  wf.setnchannels(CHANNELS)
  wf.setsampwidth(audio.get_sample_size(FORMAT))
  wf.setframerate(RATE)
  wf.writeframes(b''.join(frames))
  wf.close()

  # STT Conversion
  with open(OUTPUT_FILE, 'rb') as audioFile:
    question = openAIClient.audio.transcriptions.create(model = 'whisper-1', file = audioFile).text
  Path(OUTPUT_FILE).unlink()
```

---
### Object Recognition: `lookForObjects()`

Study-Bot has can identify objects using 3 different methods: color ranges, ArUco markers, and a custom computer vision model, depending on which is more feasable to implement for a specific topic and its set of educational materials.

The function `lookForObjects()` is called when the user asks a question, and it will populate the object list with whatever they are holding. This list is a string containing natural language, as it needs to be read by **GPT**. The function `colorID()` is used to detect the *some* of the pieces of the human body set based on predefined color ranges, the function `markerID()` is used to detect the objects for the Krebs cycle set based on the ArUco markers attached to each one of the pieces of the set, and the function `modelID()` is used to detect the objects using the custom computer vision model trained with all the pieces in the human body set.

```python
# Takes the topic number and camera number as arguments, if no camera number is provided, the default camera is used
def lookForObjects(topic: int, camera: int = 0):
  if topic == 1:
		
		colorID(camera)
	elif topic == 2:
		
		markerID(camera)
	elif topic == 3:
		
		modelID(camera)
```

**Color Identification:** `colorID()`

This function uses [OpenCV](https://opencv.org/) to detect objects that fit into the predefined color ranges. It will look for objects for one second, and if it finds any match, it will add it to the object list. If it doesn't find anything, the variable will contain the default message, set at the start of the function.

For each of the objects you wish to identify, set a lower and upper bound with the lightest and darkest values for the color of the object in the **HSV color space**. You may use a color picker tool to obtain these values. Consider that the effectiveness of these values will depend on the lighting conditions of the environment, color accuracy of the camera, white balance, and other factors.

>Note: For **OpenCV**, the H (hue) value is represented as a value between 0 and 180, instead of the standard 0 to 360, and the S (saturation) and V (vibrancy) values are represented as values between 0 and 255 instead of 0 to 100.

<!-- Use white balance of 3154 for our camera -->
```python
def colorID(camera: int = 0):
  global objects
  objects = 'User is not holding any objects'

  # Capture video
  cam = cv2.VideoCapture(camera, cv2.CAP_DSHOW) # Use 0 for default camera

  # Start timer
  startTime = time.time()
  elapsedTime = 0

  # Color ranges
  stomachLower = np.array([90,   80,     100], np.uint8)
  stomachUpper = np.array([120,  255,    255], np.uint8)
  colonLower = np.array(  [10,   140.25, 89.25], np.uint8)
  colonUpper = np.array(  [19.5, 255,    255], np.uint8)
  liverLower = np.array(  [38,   49.5,   96.9], np.uint8)
  liverUpper = np.array(  [41,   255,    255], np.uint8)
  brainLower = np.array(  [161,  127.5,  102], np.uint8)
  brainUpper = np.array(  [161,  255,    255], np.uint8)
  kidneyLower = np.array( [26,   153,    175.95], np.uint8)
  kidneyUpper = np.array( [26,   255,    255], np.uint8)
  heartLower = np.array(  [177,  127.5,  89.25], np.uint8)
  heartUpper = np.array(  [177,  247.35, 175.95], np.uint8)

  while elapsedTime < 1:
    _, imageFrame = cam.read()

    # Convert frame from BGR color space to HSV
    hsvFrame = cv2.cvtColor(imageFrame, cv2.COLOR_BGR2HSV)

    # Create masks for each organ
    colonMask = cv2.inRange(hsvFrame, colonLower, colonUpper)
    liverMask = cv2.inRange(hsvFrame, liverLower, liverUpper)
    stomachMask = cv2.inRange(hsvFrame, stomachLower, stomachUpper)
    brainMask = cv2.inRange(hsvFrame, brainLower, brainUpper)
    kidneyMask = cv2.inRange(hsvFrame, kidneyLower, kidneyUpper)
    heartMask = cv2.inRange(hsvFrame, heartLower, heartUpper)
```

When initializing the camera, using the `CAP_DSHOW` flag reduces the time it takes to open the camera by a considerable amount. This is because the flag tells OpenCV to use a different API to access the camera, which is faster than the default API.

For each object, create binary masks where color-matched pixels are white and non-matching are black. To minimize false negatives, apply a dilation morphological transformation. This enlarges object boundaries in the binary image by sliding a structuring element (kernel) over the image and replacing each pixel with the maximum pixel value within the kernel's neighborhood.

A 5x5 square-shaped kernel was used for most organs, and a 12x12 for the kidney, since it was harder to detect its specific color range, filling any gaps in the mask.

```python
    # Create a 5x5 square-shaped filter called kernel
    # Filter is filled with ones and will be used for dilation
    kernel = np.ones((5, 5), 'uint8')

    # For colon
    # Dilate mask: Remove holes in the mask by adding pixels to the boundaires of the objects in the mask
    colonMask = cv2.dilate(colonMask, kernel)
    stomachMask = cv2.dilate(stomachMask, kernel)
    brainMask = cv2.dilate(brainMask, kernel)
    heartMask = cv2.dilate(heartMask, kernel)
    # Use larger kernels for heart, liver, and kidney masks
    liverMask = cv2.dilate(liverMask, np.ones((12, 12), 'uint8'))
    kidneyMask = cv2.dilate(kidneyMask, np.ones((13, 13), 'uint8'))
    heartMask = cv2.dilate(heartMask, np.ones((12, 12), 'uint8'))

```

Create a contour around the zone that matches the color range of the object. The binary masks are used as the input to the `cv2.findContours()` function to find contours in the image that correspond to the color range of the object.

The function will return a list of contours, iterate over each one of them, determine its `area`, and compare ir to a predefined **size threshold**.

If the `area` is greater than the threshold, it indicates that the detected region is significant enough to be considered a positive detection for the object, so its name is added to the list, while avoiding repeats. This operation is performed for each object of the set.

```python
    # Create a contour around the zone that matches the color range
    contours, _ = cv2.findContours(colonMask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)

    # For each contour, check if the area is greater than the threshold
    for contour in contours:
      area = cv2.contourArea(contour)
      if area > 700 and 'colon' not in objects:
        # Append the name of the model to the list of objects
        objects = f'{objects}, colon' if objects != 'User is not holding any objects' else 'colon'

    contours, _ = cv2.findContours(liverMask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)
    for contour in contours:
      area = cv2.contourArea(contour)
      if area > 1300 and 'liver' not in objects:
        objects = f'{objects}, liver' if objects != 'User is not holding any objects' else 'liver'

    contours, _ = cv2.findContours(stomachMask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)
    for contour in contours:
      area = cv2.contourArea(contour)
      if area > 1400 and 'stomach' not in objects:
        objects = f'{objects}, stomach' if objects != 'User is not holding any objects' else 'stomach'

    contours, _ = cv2.findContours(brainMask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)
    for contour in contours:
      area = cv2.contourArea(contour)
      if area > 3000 and 'brain' not in objects:
        objects = f'{objects}, brain' if objects != 'User is not holding any objects' else 'brain'
    
    contours, _ = cv2.findContours(heartMask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)
    for contour in contours:
      area = cv2.contourArea(contour)
      if area > 1400 and 'heart' not in objects:
        objects = f'{objects}, heart' if objects != 'User is not holding any objects' else 'heart'

    contours, _ = cv2.findContours(kidneyMask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)
    for contour in contours:
      area = cv2.contourArea(contour)
      if area > 50 and 'kidney' not in objects:
        objects = f'{objects}, kidney' if objects != 'User is not holding any objects' else 'kidney'
```

To calibrate this method of object detection to a specific set of objects, use the script located in `Tools/colorDetection.py` to find the color ranges that work best for the set of models and test using different `area` thresholds.

A properly calibrated set of color ranges and area thresholds will yield a level of detection accuracy resembling the image below, in which there are no false positives between similar colors and the amount of areas detected (or rectangles shown) for each object is close or equal to 1.

<p align="center">
  <img src="Images/Color Detection.png" alt="Color Detection" width="550"/>
</p>

>Image generated using the `colorDetection.py` script, which added rectangles around the areas of the image that met the criteria. These educational models are 3D prints representing some of the organs of the human body.

**ArUco Marker identification:** `markerID()`  

Consider that the ArUco module is not included in the default OpenCV installation, so you need to install a specific module named `opencv-contrib-python`. This is included in the `requirements.txt` file, but you can install it manually as follows: 

```bash
pip install opencv-contrib-python
```

This function uses **OpenCV's** ArUco module to detect the ArUco markers attached to the objects of a set. Similar to `colorID()`, it will look for the markers for *5* seconds, and if it finds any, it will add to the `objects` variable what it has found, and will leave the default message if it doesn't find anything.

**OpenCV** has predefined dictionaries of ArUco markers, and the `DICT_4X4_50` dictionary is used here, which contains 50 different markers of 4x4 bits. To obtain the images with the markers, refer to the script in `Tools/ArUcoCreate.py`.

Select a dictionary and asociate the number of an ArUco marker with the name of the object it is attached to:

```python
def markerID():
  obj = 'User is not holding any objects'

  # Choose the predefined dictionary to use
  arucoDict = cv2.aruco.getPredefinedDictionary(cv2.aruco.DICT_4X4_50)

  # Define the names of the objects
  compoundDict = { 0: 'Citrate', 1: 'Isocitrate', 2: 'Alpha-Ketoglutarate', 3: 'Succinyl CoA', 4: 'Succinate', 5: 'Fumarate', 6: 'Malate', 7: 'Oxaloacetate' }

  cap = cv2.VideoCapture(0, cv2.CAP_DSHOW) # Use 0 for default camera

  # Start timer
  startTime = time.time()
  elapsedTime = 0
```

For each captured frame, convert it to grayscale and use the `cv2.aruco.detectMarkers()` function to get a matrix of the detected markers and their corresponding IDs. If any markers are detected, iterate over the matrix and add the name of the object to the list, while avoiding repeats.

```python
  while elapsedTime < 5:
    ret, frame = cap.read()

    if not ret:
      print('Failed to capture frame.')
      break

    # Convert the frame to grayscale for marker detection
    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)

    # Detect markers
    corners, ids, _ = cv2.aruco.detectMarkers(gray, arucoDict)

    if ids is not None:
      # For each marker found in current frame
      for i in range(len(ids)):
        try:
          # Try to get the name of the compound from the dictionary
          compoundName = compoundDict[ids[i][0]]
          # Append compound to list while avoiding repeats
          if obj == 'User is not holding any objects':
            obj = compoundName
          elif compoundName not in obj:
            obj += ', ' + compoundName
        except KeyError:
          continue

    # Display the frame
    cv2.imshow('Study-Bot View', frame)

    elapsedTime = time.time() - startTime

    # Check for 'Esc' key press
    key = cv2.waitKey(10) & 0xFF
    if key == 27:
      break
```

In this case the frame is displayed since it is more important to have the camera feed visible to the user, or the person assisting them, to make sure that the objects are correctly aligned with the camera when scanning for markers rather than detecting colors.

---
### Answer Generation: `sendMessage()`

This function simply takes a message list as input and sends it to the **OpenAI API**. Use the chat completions endpoint to allow follow up questions to previous messages. Consider that `ChatCompletions` is no longer available on the latest versions of the `openai` library. **Study-Bot** was developed using version `0.27.8`, and this specific version is indicated in the `requirements.txt` file; it can also be installed manually as:

```bash
pip install openai==0.27.8
```

The `temperature` parameter is used to control the 'randomness' of the response. A lower temperature value will result in more predictable responses, as the model is said to take a more objective and factual approach to the response.

After the answer is extracted from the API's JSON response, it is appended to `messageList` globally. Note that the answer is saved as a dictionary with the role of the message (`assistant` or `user`), as this is important information for the model to understand who sent which message when generating a response.

```python
def sendMessage(messageList: any):
  # Send prompt to GPT
  response = openai.ChatCompletion.create(
    messages = messageList,
    model = GPT_MODEL, 
    temperature = 0.2
  )
  # print(response)
  _answer = response['choices'][0]['message']['content']

  # Add the response to the message list
  messageList.append({'role': 'assistant', 'content': _answer})
```
---
### Text to Speech: `convertTTS()` and `streamAnswer()`

To convert **GPT's** text answer into speech, call the **Elevenlabs API** to generate the audio. Here, the `eleven_multilingual_v1` model is used, which automatically detects the language of the input text and generates the audio in the same language as long as it is supported. 

The `stream` parameter is set as True to play the audio as it is being generated, instead of waiting for the entire text to be converted to begin audio playback. The audio is streamed to the `streamAnswer()` function, which plays the audio using the `pydub` library.

>Note: There is a function in the **Elevenlabs** library for audio streaming, however, a different one is needed since the library's function requires the use of **mpv**, a command line media player. While it might be the better option for using this API through Python, it is not ideal for our specific use case, since we want non-technical users to be able to use the program without installing *too many* additional dependencies.

```python
def streamAnswer(audioStream: Iterator[bytes]) -> bytes:
  audioOutput = b''
  
  # For each chunk of audio in stream, add it to the output
  for chunk in audioStream:
    if chunk is not None:
      audioOutput += chunk

  # Play audio output using PyDub
  audioSegment = AudioSegment.from_file(io.BytesIO(audioOutput), format="mp3")
  pydubPlay(audioSegment)

def convertTTS(text: str):
  audioOutput = generate(text = text, model = 'eleven_multilingual_v1', stream = True)
  streamAnswer(audioOutput)
```
---
### Start Conversation and Use of Multithreading

This section of the code is only executed when `studyBot.py` is run as a standalone script. 

Use this as a topic selector to choose the source material for the conversation:

```python
  # Listen for keyboard input to stop recording
  keyboard.add_hotkey('s', stopRecording)

  print('Select a topic NUMBER from the list:\n')
  print('[1] - Human Body')
  print('[2] - Krebs Cycle\n')
  topic = int(input('Topic: '))
  source = ''

  # Load the source material based on the selected topic
  if topic == 1:
    print('Topic: Human Body\n')
    source = sourceMaterial.humanBody
  elif topic == 2:
    print('Topic: Krebs Cycle\n')
    source = sourceMaterial.krebsCycle
```

Transcribing audio into text and detecting objects are very time-consuming tasks. To avoid having to wait for one to finish before the other can start, use **multithreading** to run both processes simultaneously. Create two threads for these processes and wait for them to finish before continuing with the rest of the program.

```python
  # Start question processing threads
  objID = threading.Thread(target = lookForObjects, args = (topic,))
  audioRec = threading.Thread(target = recordQuestion)

  objID.start()
  print('Looking for objects...\n')
  audioRec.start()
  print('Listening for question...\n')

  objID.join()
  print('Object detection complete.\n')
  print('Objects detected: ' + objects + '\n')
  audioRec.join()
  print('Question recorded.\n')
  print('Question: ' + question + '\n')
```

After the information is gathered, build the initial query. Since this is the very first message sent to **GPT**, it includes the behavioral guidelines in `instructions` and the source material of the set of models. 

```python
  # Build prompt
  query = f"""{instructions}

  Objects held by user: {objects}.
  Question: {question}

  Information: 
  \"\"\"
  {source}
  \"\"\"
  """
```
Notice the role parameters on the messages in `messageHistory`. The very first message has the `system` role, and serves as some initial context or instructions about the task that **GPT** will be performing in this conversation.

```python
  # Send prompt to GPT
  messageHistory = [
    {'role': 'system', 'content': 'You answer questions in the same language as the question.'},
    {'role': 'user', 'content': query},
  ]

  print('Sending prompt to GPT...\n')
  sendMessage(messageHistory)
  # Get the answer from the last message in the message history
  answer = next((msg for msg in reversed(messageHistory) if msg['role'] == 'assistant'), None)['content']
  print(answer + '\n')
  
  if answer != '':
    print('Answer: ' + answer + '\n\n')

  # Convert answer to audio
  print('Converting answer to audio...\n')
  convertTTS(answer)
```
---
### Conversation Handling

After this initial message, the conversation is handled in a loop. The process is pretty straightforward and similar to the first message, with the main difference being the use of the keyboard to trigger the question processing, and in the use of the `sendMessage()` function, we append only the new messages without any need for additional context.

```python
  # Conversation loop, handles any follow-up questions
  while True:
    print('Press space to ask another question, or press q to quit.\n')

    while True:
      if keyboard.is_pressed(' '):
        print('Preparing for next question, please hold...\n')
        break
      if keyboard.is_pressed('q'):
        print('Exiting program...\n')
        exit()

    # Reset variables
    objects = 'User is not holding any objects'
    question = ''

    # Restart threads

    objID = threading.Thread(target = lookForObjects, args = (topic,))
    audioRec = threading.Thread(target = recordQuestion)

    objID.start()
    print('Looking for objects...\n')
    audioRec.start()
    print('Listening for question...\n')

    objID.join()
    print('Object detection complete.\n')
    print('Objects detected: ' + objects + '\n')
    audioRec.join()
    print('Question recorded.\n')
    print('Question: ' + question + '\n')

    # Build new prompt and add to chat history
    query = f"""Objects held by user: {objects}.
Question: {question}
"""
    messageHistory.append({'role': 'user', 'content': query})
    answer = ''

    # Send prompt to GPT
    # print('Prompt: ' + query + '\n') # For debugging only
    print('Sending prompt to GPT...\n')

    sendMessage(messageHistory)
    answer = next((msg for msg in reversed(messageHistory) if msg['role'] == 'assistant'), None)['content']

    if answer != '':
      print('Answer: ' + answer + '\n\n')

    print('Converting answer to audio...\n')
    convertTTS(answer)
```
